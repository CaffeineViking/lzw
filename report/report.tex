\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[a4paper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pdftex, hidelinks,
            pdftitle={Implementing Several Lossless Compression Algorithms
                      Adaptive Arithmetic Coding (BAC) and the LZW for C++},
            pdfauthor={{Erik Sven Vasconcelos Jansson}},
            pdfsubject={Data Compression -- Algorithms},
            pdfkeywords={lossless, data compression, algorithm,
                         entropy, estimation, c++, lzw, coding,
                         adaptive, arithmetic coding}]{hyperref}

\usepackage{bm}
\usepackage{caption}
\usepackage{relsize}
\usepackage{csquotes}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{algorithmic}
\usepackage{todonotes}
\usepackage{graphicx}
\usepackage{courier}
\usepackage{acronym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{siunitx}
\usepackage{algorithm}
\usepackage[capitalize, noabbrev]{cleveref}
\usepackage[activate={true, nocompatibility}, final,
            tracking=true, kerning=true, spacing=true,
            factor=1100, stretch=10, shrink=10]{microtype}

\newcommand\CC{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\relsize{-3}{\textbf{++}}}}

\DeclareCaptionFormat{modifiedlst}{\rule{\textwidth}{0.85pt}\\[-2.9pt]#1#2#3}
\captionsetup[lstlisting]{format =  modifiedlst,
labelfont=bf,singlelinecheck=off,labelsep=space}
\lstset{basicstyle=\footnotesize\ttfamily,
        breakatwhitespace = false,
        breaklines = true,
        keepspaces = true,
        language = C++,
        showspaces = false,
        showstringspaces = false,
        frame = tb,
        numbers = left,
        numbersep = 5pt,
        xleftmargin = 16pt,
        framexleftmargin = 16pt,
        belowskip = \bigskipamount,
        aboveskip = \bigskipamount,
        escapeinside={<@}{@>}}

\title{\textbf{\makebox[\linewidth][s]{Implementing Several Lossless Compression Algorithms:}
             \\\makebox[\linewidth][s]{Adaptive Arithmetic Coding (AC) and the LZW for \CC}}}
\author{{\textbf{Erik Sven Vasconcelos Jansson}} \\
        {\href{mailto:erija578@student.liu.se}
        {\texttt{<erija578@student.liu.se>}}} \\
        {Link√∂ping University, Sweden}}

\begin{document}
    \maketitle

    \section*{Abstract}

    Data compression provides essential methods when one wants to store or transfer information efficiently. In this short technical report, common techniques for modelling and two different coders are presented. By measuring the entropy of the Canterbury corpus, the limits of coding can be determined for a source. Both the \emph{Lempel--Ziv--Welch} and \emph{Arithmetic Coding} are briefly described and then implemented, where common practical problems \& solutions are shown. Finally, measurements of both \emph{rate} and \emph{speed} for these methods are compared to the actual entropy.

    \vspace{-0.1in}
    \pagenumbering{gobble}
    \tableofcontents
    \newpage

    Since this article is quite short, further details on \emph{Arithmetic Coding}, \emph{LZW}, \emph{ANS} can be found below:

    \nocite{*} % Include all.
    \bibliographystyle{abbrv}
    \bibliography{report}

    \begin{table*}[t]
        \centering
        \begin{tabular}{lllll}
        \toprule
            Source's Filename & Size in Bytes & $\mathrm{H}(S_n)$ in Shannons & $\mathrm{H}(S_n | S_{n-1})$ & $\mathrm{H}(S_n | S_{n-1}, S_{n-2})$ \\
        \midrule
            \texttt{alice29.txt} & 152089 & 4.567721306757149 & 3.418682225075357 & 2.48520469580814 \\
            \texttt{asyoulik.txt} & 125179 & 4.808169990765448 & 3.417731826885349 & 2.53820221279907 \\
            \texttt{bible.txt} & 4047392 & 4.342752822300183 & 3.269099339823096 & 2.47861800400018 \\
            \texttt{cp.html} & 24603 & 5.229444511631613 & 3.467709563850320 & 1.73841659079717 \\
            \texttt{E.coli} & 4638690 & 1.999821485762563 & 1.981417241665094 & 1.96323532309116 \\
            \texttt{fields.c} & 11150 & 5.008337632132268 & 2.950926540872756 & 1.47064554512717 \\
            \texttt{grammar.lsp} & 3721 & 4.633982742963610 & 2.806478773539829 & 1.28692014638656 \\
            \texttt{kennedy.xls} & 1029744 & 3.573474997044099 & 2.777314597758521 & 1.71153686785860 \\
            \texttt{lcet10.txt} & 426754 & 4.669132957659401 & 3.497027811530963 & 2.61231530515721 \\
            \texttt{plrabn12.txt} & 481861 & 4.531375618032128 & 3.366065910049050 & 2.71692784135892 \\
            \texttt{ptt5} & 513216 & 1.210175035218506 & 0.823658947303357 & 0.70519976712624 \\
            \texttt{sum} & 38240 & 5.329193324866448 & 3.298284974946003 & 1.93087067303595 \\
            \texttt{world192.txt} & 2473400 & 4.998316616349440 & 3.660472085185258 & 2.77064872349497 \\
            \texttt{xargs.1} & 4227 & 4.900067215713765 & 3.196483510458247 & 1.55061392381553 \\
        \bottomrule
        \end{tabular}
        \caption{Entropy Estimations (0$^{th}$, 1$^{st}$ and 2$^{nd}$ Markov Orders) for the Canterbury Corpus Test Set}
        \label{tab:canterbury}
    \end{table*}

    \section{Source Modelling} \label{sec:source_modelling}

        Before \emph{source coding} anything, formulation of the source's \emph{statistical properties} needs to be \emph{modelled}. Typically, ones which \emph{predict future symbols} better. Below we define two useful models, which will be used to \emph{estimate entropies} in the \emph{Canterbury corpus}.

        \subsection{Stationary Source} \label{sec:stationary_source}

        Generates a \emph{sequence} \(S\) of \emph{symbols} \(\{s_i\}\) within some \emph{alphabet} \(\mathcal{A}\). Symbols are \emph{independent} of each other, knowing past symbols doesn't affect the current \(s_n\): \[p(s_n | s_{n-1}, \dots, s_{1}) = p(s_n),\, s_i \in \mathcal{A}\]

        \subsection{Markov Information Source} \label{sec:markov_information_source}

        However, in contrast, some sources \emph{do} generate \(S\)'s in which \(s_n\) \emph{depends} on the \(k^{th}\) past symbols. Such a source has \emph{memory} up to \emph{Markov order \(k\)}. Formally: \[p(s_n | s_{n-1}, \dots, s_{1}) = p(s_n | s_{n-1}, \dots, s_{n-k})\]

        \subsection{Estimating Source Entropy} \label{sec:estimating_source_entropy}

        Within \emph{lossless data compression}, the best \emph{rate of compression} achievable is the \emph{Shannon entropy}~\cite{sayood2012introduction}. Accurate modelling gives the source's \emph{optimal rate}:
        \begin{equation} \label{eq:entropy}
            H(S) = -\sum_i p(s_i) \log_b p(s_i)
        \end{equation}

        Finally, we attempt to \emph{estimate} the entropies of the sources within the Canterbury corpus. First, we estimate the \emph{joint probabilities} using Algorithm~\ref{alg:entropy}, which measures a symbol's frequency up to order \(k\).

        \begin{algorithm}
            \begin{algorithmic}
                \REQUIRE Sequence of \(|S|\) symbols,\: where \(s_i \in \mathcal{A}.\)
                \FOR{\(i \leftarrow 1\) \TO \(|S|\)}
                    \FOR{\(j \leftarrow 0\) \TO \(k\)}
                        \STATE{\(\mathrm{state} \leftarrow (s_{i-j}, s_{i-j-1}, \dots, s_{i})\)}
                        \STATE{\(f_{\mathrm{state}} \leftarrow f_{\mathrm{state}} + 1\)}
                    \ENDFOR
                \ENDFOR

                \FORALL{state \(\in f\)}
                    \STATE{\(\mathrm{total} \leftarrow |S| - |\mathrm{state}| - 1\)}
                    \STATE{\(p(\mathrm{state}) \leftarrow f_\mathrm{state} \div \mathrm{total}\)}
                \ENDFOR
            \end{algorithmic}
            \caption{Estimating \(k^{th}\) Source Probabilities}
            \label{alg:entropy}
        \end{algorithm}

        Having done this, we calculate the \emph{joint entropy}, which is utilized in the \emph{conditional entropy} since we have already calculated \(H(S_n, \bm{S})\) and \(H(\bm{S})\), where we define \(\bm{S} \equiv S_{n-1}, \dots, S_{n-k},\; \bm{s} \equiv s_{n-1}, \dots, s_{n-k} \):
        \begin{gather} \label{eq:entropies}
            H(S_n, \bm{S}) = - \sum_n \cdots \sum_k p(s_n, \bm{s}) \log_b p(s_n, \bm{s}) \\
            H(S_n | \bm{S}) = H(S_n, \bm{S}) + H(\bm{S})
        \end{gather}

        By using the implementation in Appendix~\ref{sec:script}, we can produce Table~\ref{tab:canterbury} which has the \(0^{th}, 1^{st}\) and \(2^{nd}\) order conditional entropies of each source in the set.

    \begin{table*}[t]
        \centering
        \begin{tabular}{lllll}
        \toprule
            Source's Filename & Size in Bytes & Rate in Bits/Symb & Encoding Millisec & Decoding Millisec \\
        \midrule
            \texttt{alice29.txt}  &   70148 & 3.689839501870608 & 35.2187500000000 & 16.9062500000000 \\
            \texttt{asyoulik.txt} &   62748 & 4.010129494563784 & 30.5937500000000 & 14.6562500000000 \\
            \texttt{bible.txt}    & 1501134 & 2.967113637621456 & 303.250000000000 & 118.062500000000 \\
            \texttt{cp.html}      &   14948 & 4.860545461935536 & 8.12500000000000 & 7.12500000000000 \\
            \texttt{E.coli}       & 1342680 & 2.315619280443400 & 318.437500000000 & 77.7500000000000 \\
            \texttt{fields.c}     &    7084 & 5.082690582959640 & 5.21875000000000 & 4.09375000000000 \\
            \texttt{grammar.lsp}  &    2818 & 6.058586401504968 & 3.56250000000000 & 3.31250000000000 \\
            \texttt{kennedy.xls}  &  332902 & 2.586289407852824 & 163.562500000000 & 111.468750000000 \\
            \texttt{lcet10.txt}   &  185158 & 3.471002029272128 & 84.3125000000000 & 79.4687500000000 \\
            \texttt{plrabn12.txt} &  220976 & 3.668709441104384 & 102.718750000000 & 87.1875000000000 \\
            \texttt{ptt5}         &   70116 & 1.092966704077808 & 66.7812500000000 & 20.6875000000000 \\
            \texttt{sum}          &   25054 & 5.241422594142256 & 8.90625000000000 & 7.87500000000000 \\
            \texttt{world192.txt} & 1075670 & 3.479162286730808 & 206.125000000000 & 83.1562500000000 \\
            \texttt{xargs.1}      &    3584 & 6.783061272770280 & 3.81250000000000 & 5.12500000000000 \\
        \bottomrule
        \end{tabular}
        \caption{Lempel-Ziv-Welch Coder (``Markov'' Model-ish) Results in the Canterbury Corpus Test Set}
        \label{tab:lzw}
    \end{table*}

    \section{Lempel--Ziv--Welch Coding} \label{sec:lempel_ziv_welch}

        Another \emph{dictionary coder} based off \emph{Lempel--Ziv's 78} coding algorithm, introduced by \emph{Welch}~\cite{welch1984technique} in 1984. However, due to patent issues, it remained unused, and the \emph{deflate coder} was usually a good alternative. Nowadays, these patents have expired, and it's used in e.g.\ the \emph{Unix} \texttt{compress} tool, see \emph{Nieminen}~\cite{nieminen2007efficient}, but in practice, it's generally not used very much...

        \vspace{-0.1in}
        \subsection{Encoding and Decoding} \label{sec:lzw_encoding_decoding}

        As with other \emph{dictionary coders}, we seek \emph{matches} in the sequence to be coded and the \emph{dictionary}, which is built \emph{adaptively} (as we go) with previous matches. Instead of coding the matching sequence, we write the \emph{location} of the sequence within the dictionary.

        Below follow the encoding \& decoding algorithms:

        \begin{algorithm}
            \begin{algorithmic}
                \STATE{\(\mathrm{prefix} \leftarrow \epsilon\)}
                \STATE{\(\mathrm{dictionary}_i \leftarrow \epsilon s_i,\, \forall s_i \in \mathcal{A}\)}
                \WHILE{\(s_k \leftarrow \mathrm{read(\emph{input})} \neq \varnothing\)}
                    \STATE{\(\mathrm{string} \leftarrow \mathrm{append(prefix,}\, s_k)\)}
                    \IF{\(\mathrm{index} \leftarrow \mathrm{find(string, dictionary)} = \varnothing \)}
                        \STATE{\(\mathrm{write(find(prefix, dictionary), \emph{output})}\)}
                        \STATE{\(\mathrm{dictionary}_j \leftarrow \mathrm{string}\; ;\; \mathrm{prefix} \leftarrow s_k\)}
                    \ELSE
                        \STATE{\(\mathrm{prefix} \leftarrow \mathrm{string}\)}
                    \ENDIF
                \ENDWHILE
            \end{algorithmic}
            \caption{Lempel--Ziv--Welch Encoding Steps}
            \label{alg:lzwz}
        \end{algorithm}

        \newpage

        \begin{algorithm}
            \begin{algorithmic}
                \STATE{\(\mathrm{past} \leftarrow \epsilon\)}
                \STATE{\(\mathrm{dictionary}_i \leftarrow \epsilon s_i,\, \forall s_i \in \mathcal{A}\)}
                \WHILE{\(c_k \leftarrow \mathrm{read(\emph{input})} \neq \varnothing\)}
                    \IF{\(\mathrm{string} \leftarrow \mathrm{exists(} c_k, \mathrm{dictionary}) = \varnothing\)}
                        \STATE{\(\mathrm{dictionary}_j \leftarrow \mathrm{append(past, past}_k)\)}
                        \STATE{\(\mathrm{write(append(past, past}_k\mathrm{),\, \emph{output})}\)}
                    \ELSE
                        \STATE{\(\mathrm{dictionary}_j \leftarrow \mathrm{append(past, string}_k)\)}
                        \STATE{\(\mathrm{write(string, \emph{output})} \; ;\; \mathrm{past} \leftarrow \mathrm{string}\)}
                    \ENDIF
                \ENDWHILE
            \end{algorithmic}
            \caption{Lempel--Ziv--Welch Decoding Steps}
            \label{alg:lzwx}
        \end{algorithm}
        \vspace{-0.2in}
        \subsection{Implementing Coder} \label{sec:lzw_implementing_coder}

        Several practical details were left out above, these are covered in \emph{Nieminen's}~\cite{nieminen2007efficient} excelled article. Main modifications are to \emph{``chain together''} entries in the dictionary, meaning we avoid storing entire strings. Another detail is how to efficiently find a string in the dictionary, here we use a \emph{binary search tree}, exploiting its structure, needs \(\leq\lg 256\) comparisons.

        See Appendix~\ref{sec:liblzw} for the implementation, we use \(16\)-bit fixed codewords for the \(2^{16}\) dictionary entries, requiring 576 KiB when full (then, resetting itself).

        \vspace{-0.146in}
        \subsection{Rate and Speed} \label{sec:lzw_rate_and_speed}

        Comparing Table~\ref{tab:canterbury} and Table~\ref{tab:lzw}, we see that LZW performs poorly on small files. This is because the dictionary hasn't even had time to become half full. We also see it performs well on large natural texts.

    \begin{table*}[t]
        \centering
        \begin{tabular}{lllll}
        \toprule
            Source's Filename & Size in Bytes & Rate in Bits/Symb & Encoding Millisec & Decoding Millisec \\
        \midrule
            \texttt{alice29.txt}  &   87653 & 4.610616152384456 & 118.375000000000 & 111.312500000000 \\
            \texttt{asyoulik.txt} &   75794 & 4.843879564463680 & 87.1562500000000 & 106.906250000000 \\
            \texttt{bible.txt}    & 2220355 & 4.388712533898368 & 867.375000000000 & 882.562500000000 \\
            \texttt{cp.html}      &   16306 & 5.302117627931552 & 42.5937500000000 & 49.0312500000000 \\
            \texttt{E.coli}       & 1173737 & 2.024255986065024 & 630.375000000000 & 633.000000000000 \\
            \texttt{fields.c}     &    7156 & 5.134349775784752 & 32.7812500000000 & 24.3437500000000 \\
            \texttt{grammar.lsp}  &    2297 & 4.938457403923672 & 12.7812500000000 & 12.7500000000000 \\
            \texttt{kennedy.xls}  &  482256 & 3.746608865892880 & 214.750000000000 & 217.937500000000 \\
            \texttt{lcet10.txt}   &  257791 & 4.832592078808864 & 185.812500000000 & 185.812500000000 \\
            \texttt{plrabn12.txt} &  274853 & 4.563191459777816 & 197.531250000000 & 187.593750000000 \\
            \texttt{ptt5}         &  117098 & 1.825321112358144 & 116.906250000000 & 149.656250000000 \\
            \texttt{sum}          &   28393 & 5.939958158995808 & 64.4687500000000 & 56.3437500000000 \\
            \texttt{world192.txt} & 1573387 & 5.088985202555184 & 588.937500000000 & 594.062500000000 \\
            \texttt{xargs.1}      &    2735 & 5.176247929973976 & 13.2812500000000 & 13.4687500000000 \\
        \bottomrule
        \end{tabular}
        \caption{Adaptive Arithmetic Coder (Stationary Model) Results in the Canterbury Corpus Test Set}
        \label{tab:aac}
    \end{table*}

    \section{Adaptive Arithmetic Coding} \label{sec:adaptive_arithmetic_coding}
        An \emph{entropy encoder} attempts to assign shorter codewords to symbols that are more probable, leading to compression. \emph{Arithmetic coding} was popularized by \emph{Witten et al.}~\cite{witten1987arithmetic} in 1987, and has become central in both \emph{lossless} and \emph{lossy compression} because the coder separates modelling from the coding step and also because it provides rates coverging to entropy.

        \vspace{-0.1in}
        \subsection{Encoding and Decoding} \label{sec:aac_encoding_decoding}

        Conceptually, the idea is simple. Assign an interval \([l_k, u_k[\) which is unique to each symbol \(s_k\), where \(u_k - l_k\) is proportional to \(p(s_k)\). This is usually done with the \emph{cumulative probability}: \(F(k) = \sum_{i=1}^k p(s_i)\), or if adaptive, the \emph{empirical frequency} of the symbol.

        Sequences are encoded by continuously ``zooming'' into the interval \([0, 1[\) with \(F(k-1)\) and \(F(k)\), by re-scaling the initial interval. Any number between the final interval can be chosen, usually: \((u + l) \div 2\).

        \begin{algorithm}
            \begin{algorithmic}
                \STATE{\(\mathrm{lower} \leftarrow 0.0\; ;\; \mathrm{upper} \leftarrow 0.0\)}
                \WHILE{\(s_k \leftarrow \mathrm{read(\emph{input})} \neq \varnothing\)}
                    \STATE{\(\mathrm{range} \leftarrow \mathrm{upper} - \mathrm{lower}\)}
                    \STATE{\(\mathrm{upper} \leftarrow \mathrm{lower} + \mathrm{range} \cdot F(k)\)}
                    \STATE{\(\mathrm{lower}\: \leftarrow \mathrm{lower} + \mathrm{range} \cdot F(k-1)\)}
                \ENDWHILE
                \STATE{\(\mathrm{write(} (\mathrm{lower} + \mathrm{upper}) \div 2, \mathrm{\emph{output})}\)}
            \end{algorithmic}
            \caption{Adaptive Arithmetic Encoding Steps}
            \label{alg:aacz}
        \end{algorithm}

        \newpage

        \begin{algorithm}
            \begin{algorithmic}
                \STATE{\(c \leftarrow \mathrm{read(\emph{input})}\)}
                \WHILE{\(s_k \neq \varnothing\)}
                    \STATE{\(s_k\, \leftarrow s_k,\: F(k-1) \leq c \leq F(k)\)}
                    \STATE{\(\mathrm{upper} \leftarrow F(k)\; ;\; \mathrm{lower} \leftarrow F(k-1)\)}
                    \STATE{\(\mathrm{range}\;\, \leftarrow \mathrm{upper} - \mathrm{lower}\)}
                    \STATE{\(c \leftarrow (c - \mathrm{lower}) \div \mathrm{range}\)}
                    \STATE{\(\mathrm{write(} s_k \mathrm{, \emph{output})}\)}
                \ENDWHILE
            \end{algorithmic}
            \caption{Adaptive Arithmetic Decoding Steps}
            \label{alg:aacx}
        \end{algorithm}

        \vspace{-0.2in}
        \subsection{Implementing Coder} \label{sec:aac_implementing_coder}

        Unfortunately, there are several practical problems with this approach, \emph{Howard et al.}~\cite{howard1992practical}, \emph{Moffat et al.}~\cite{moffat1998arithmetic} provide a nice summary of them. First, we want to have \emph{integer arithmetic} for the \emph{symbol frequencies} instead, and because \emph{precision is limited}, we desire some way to handle \emph{infinite sequences of bits} which should also be written in \emph{small chunks} to a output.

        See Appendix~\ref{sec:libaac} for complete solutions, briefly, we \emph{estimate symbol frequencies}, \emph{re-scale} accodingly, and \emph{shift out bits} we know never will change again. Arithmetic done in 32-bits, counts stored in 16-bits.

        \vspace{-0.146in}
        \subsection{Rate and Speed} \label{sec:aac_rate_and_speed}

        Looking at Tables~\ref{tab:canterbury},\ref{tab:lzw},\ref{tab:aac}, we see that, indeed, we approach the source's (\(0^{th}\) order) entropy with AC. It performs poorly against LZW because we assume a ``Markov model'' there. LZW is considerably faster. If we were to use PPM we should outperform LZW.

    \appendix \onecolumn

    \clearpage
    \section{Source Code for the \texttt{entropy} Script} \label{sec:script}

    \lstinputlisting[language=Python, caption={{entropy.py}}]{share/entropy.py}

    \section{Source Code for the \texttt{liblzw} Library} \label{sec:liblzw}

    \lstinputlisting[caption={{lzwz.cc}}]{share/liblzw/lzwz.cc}
    \lstinputlisting[caption={{lzwx.cc}}]{share/liblzw/lzwx.cc}

    \lstinputlisting[caption={{lzw.hh}}]{share/liblzw/lzw.hh}
    \lstinputlisting[caption={{definitions.hh}}]{share/liblzw/definitions.hh}

    \lstinputlisting[caption={{buffer.hh}}]{share/liblzw/buffer.hh}
    \lstinputlisting[caption={{dictionary.hh}}]{share/liblzw/dictionary.hh}
    \lstinputlisting[caption={{dictionary.cc}}]{share/liblzw/dictionary.cc}
    \lstinputlisting[caption={{encoder.hh}}]{share/liblzw/encoder.hh}
    \lstinputlisting[caption={{encoder.cc}}]{share/liblzw/encoder.cc}
    \lstinputlisting[caption={{decoder.hh}}]{share/liblzw/decoder.hh}
    \lstinputlisting[caption={{decoder.cc}}]{share/liblzw/decoder.cc}

    \section{Source Code for the \texttt{libaac} Library} \label{sec:libaac}

    \lstinputlisting[caption={{aacz.cc}}]{share/libaac/aacz.cc}
    \lstinputlisting[caption={{aacx.cc}}]{share/libaac/aacx.cc}

    \lstinputlisting[caption={{aac.hh}}]{share/libaac/aac.hh}
    \lstinputlisting[caption={{configs.hh}}]{share/libaac/configs.hh}

    \lstinputlisting[caption={{statistics.hh}}]{share/libaac/statistics.hh}
    \lstinputlisting[caption={{statistics.cc}}]{share/libaac/statistics.cc}
    \lstinputlisting[caption={{encoder.hh}}]{share/libaac/encoder.hh}
    \lstinputlisting[caption={{encoder.cc}}]{share/libaac/encoder.cc}
    \lstinputlisting[caption={{decoder.hh}}]{share/libaac/decoder.hh}
    \lstinputlisting[caption={{decoder.cc}}]{share/libaac/decoder.cc}

\end{document}
