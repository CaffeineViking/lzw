\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[a4paper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pdftex, hidelinks,
            pdftitle={Implementing Several Lossless Compression Algorithms
                      Adaptive Arithmetic Coding (BAC) and the LZW for C++},
            pdfauthor={{Erik Sven Vasconcelos Jansson}},
            pdfsubject={Data Compression -- Algorithms},
            pdfkeywords={lossless, data compression, algorithm,
                         entropy, estimation, c++, lzw, coding,
                         adaptive, arithmetic coding}]{hyperref}

\usepackage{bm}
\usepackage{caption}
\usepackage{relsize}
\usepackage{csquotes}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{algorithmic}
\usepackage{todonotes}
\usepackage{graphicx}
\usepackage{courier}
\usepackage{acronym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{siunitx}
\usepackage{algorithm}
\usepackage[capitalize, noabbrev]{cleveref}
\usepackage[activate={true, nocompatibility}, final,
            tracking=true, kerning=true, spacing=true,
            factor=1100, stretch=10, shrink=10]{microtype}

\newcommand\CC{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\relsize{-3}{\textbf{++}}}}

\DeclareCaptionFormat{modifiedlst}{\rule{\textwidth}{0.85pt}\\[-2.9pt]#1#2#3}
\captionsetup[lstlisting]{format =  modifiedlst,
labelfont=bf,singlelinecheck=off,labelsep=space}
\lstset{basicstyle=\footnotesize\ttfamily,
        breakatwhitespace = false,
        breaklines = true,
        keepspaces = true,
        language = C++,
        showspaces = false,
        showstringspaces = false,
        frame = tb,
        numbers = left,
        numbersep = 5pt,
        xleftmargin = 16pt,
        framexleftmargin = 16pt,
        belowskip = \bigskipamount,
        aboveskip = \bigskipamount,
        escapeinside={<@}{@>}}

\title{\textbf{\makebox[\linewidth][s]{Implementing Several Lossless Compression Algorithms:}
             \\\makebox[\linewidth][s]{Adaptive Arithmetic Coding (AC) and the LZW for \CC}}}
\author{{\textbf{Erik Sven Vasconcelos Jansson}} \\
        {\href{mailto:erija578@student.liu.se}
        {\texttt{<erija578@student.liu.se>}}} \\
        {Link√∂ping University, Sweden}}

\begin{document}
    \maketitle

    \section*{Abstract}

    \pagenumbering{gobble}
    \tableofcontents
    \newpage

    Since this article is quite short, further details on \emph{Arithmetic Coding}, \emph{LZW}, \emph{ANS} can be found below:

    \nocite{*} % Include all.
    \bibliographystyle{abbrv}
    \bibliography{report}

    \begin{table*}[t]
        \centering
        \begin{tabular}{lllll}
        \toprule
            Source's Filename & Size in Bytes & $\mathrm{H}(S_n)$ in Shannons & $\mathrm{H}(S_n | S_{n-1})$ & $\mathrm{H}(S_n | S_{n-1}, S_{n-2})$ \\
        \midrule
            \texttt{alice29.txt} & 152089 & 4.567721306757149 & 3.418682225075357 & 2.48520469580814 \\
            \texttt{asyoulik.txt} & 125179 & 4.808169990765448 & 3.417731826885349 & 2.53820221279907 \\
            \texttt{bible.txt} & 4047392 & 4.342752822300183 & 3.269099339823096 & 2.47861800400018 \\
            \texttt{cp.html} & 24603 & 5.229444511631613 & 3.467709563850320 & 1.73841659079717 \\
            \texttt{E.coli} & 4638690 & 1.999821485762563 & 1.981417241665094 & 1.96323532309116 \\
            \texttt{fields.c} & 11150 & 5.008337632132268 & 2.950926540872756 & 1.47064554512717 \\
            \texttt{grammar.lsp} & 3721 & 4.633982742963610 & 2.806478773539829 & 1.28692014638656 \\
            \texttt{kennedy.xls} & 1029744 & 3.573474997044099 & 2.777314597758521 & 1.71153686785860 \\
            \texttt{lcet10.txt} & 426754 & 4.669132957659401 & 3.497027811530963 & 2.61231530515721 \\
            \texttt{plrabn12.txt} & 481861 & 4.531375618032128 & 3.366065910049050 & 2.71692784135892 \\
            \texttt{ptt5} & 513216 & 1.210175035218506 & 0.823658947303357 & 0.70519976712624 \\
            \texttt{sum} & 38240 & 5.329193324866448 & 3.298284974946003 & 1.93087067303595 \\
            \texttt{world192.txt} & 2473400 & 4.998316616349440 & 3.660472085185258 & 2.77064872349497 \\
            \texttt{xargs.1} & 4227 & 4.900067215713765 & 3.196483510458247 & 1.55061392381553 \\
        \bottomrule
        \end{tabular}
        \caption{Entropy Estimations (0$^{th}$, 1$^{st}$ and 2$^{nd}$ Markov Orders) for the Canterbury Corpus Test Set}
        \label{tab:canterbury}
    \end{table*}

    \section{Source Modelling} \label{sec:source_modelling}

        Before \emph{source coding} anything, formulation of the source's \emph{statistical properties} needs to be \emph{modelled}. Typically, ones which \emph{predict future symbols} better. Below we define two useful models, which will be used to \emph{estimate entropies} in the \emph{Canterbury corpus}.

        \subsection{Stationary Source} \label{sec:stationary_source}

        Generates a \emph{sequence} \(S\) of \emph{symbols} \(\{s_i\}\) within some \emph{alphabet} \(\mathcal{A}\). Symbols are \emph{independent} of each other, knowing past symbols doesn't affect the current \(s_n\): \[p(s_n | s_{n-1}, \dots, s_{1}) = p(s_n),\, s_i \in \mathcal{A}\]

        \subsection{Markov Information Source} \label{sec:markov_information_source}

        However, in contrast, some sources \emph{do} generate \(S\)'s in which \(s_n\) \emph{depends} on the \(k^{th}\) past symbols. Such a source has \emph{memory} up to \emph{Markov order \(k\)}. Formally: \[p(s_n | s_{n-1}, \dots, s_{1}) = p(s_n | s_{n-1}, \dots, s_{n-k})\]

        \subsection{Estimating Source Entropy} \label{sec:estimating_source_entropy}

        Within \emph{lossless data compression}, the best \emph{rate of compression} achievable is the \emph{Shannon entropy}~\cite{sayood2012introduction}. Accurate modelling gives the source's \emph{optimal rate}:
        \begin{equation} \label{eq:entropy}
            H(S) = -\sum_i p(s_i) \log_b p(s_i)
        \end{equation}

        Finally, we attempt to \emph{estimate} the entropies of the sources within the Canterbury corpus. First, we estimate the \emph{joint probabilities} using Algorithm~\ref{alg:entropy}, which measures a symbol's frequency up to order \(k\).

        \begin{algorithm}
            \begin{algorithmic}
                \REQUIRE Sequence of \(|S|\) symbols,\: where \(s_i \in \mathcal{A}.\)
                \FOR{\(i \leftarrow 1\) \TO \(|S|\)}
                    \FOR{\(j \leftarrow 0\) \TO \(k\)}
                        \STATE{\(\mathrm{state} \leftarrow (s_{i-j}, s_{i-j-1}, \dots, s_{i})\)}
                        \STATE{\(f_{\mathrm{state}} \leftarrow f_{\mathrm{state}} + 1\)}
                    \ENDFOR
                \ENDFOR

                \FORALL{state \(\in f\)}
                    \STATE{\(\mathrm{total} \leftarrow |S| - |\mathrm{state}| - 1\)}
                    \STATE{\(p(\mathrm{state}) \leftarrow f_\mathrm{state} \div \mathrm{total}\)}
                \ENDFOR
            \end{algorithmic}
            \caption{Estimating \(k^{th}\) Source Probabilities}
            \label{alg:entropy}
        \end{algorithm}

        Having done this, we calculate the \emph{joint entropy}, which is utilized in the \emph{conditional entropy} since we have already calculated \(H(S_n, \bm{S})\) and \(H(\bm{S})\), where we define \(\bm{S} \equiv S_{n-1}, \dots, S_{n-k},\; \bm{s} \equiv s_{n-1}, \dots, s_{n-k} \):
        \begin{gather} \label{eq:entropies}
            H(S_n, \bm{S}) = - \sum_n \cdots \sum_k p(s_n, \bm{s}) \log_b p(s_n, \bm{s}) \\
            H(S_n | \bm{S}) = H(S_n, \bm{S}) + H(\bm{S})
        \end{gather}

        By using the implementation in Appendix~\ref{sec:script}, we can produce Table~\ref{tab:canterbury} which has the \(0^{th}, 1^{st}\) and \(2^{nd}\) order conditional entropies of each source in the set.

    \begin{table*}[t]
        \centering
        \begin{tabular}{lllll}
        \toprule
            Source's Filename & Size in Bytes & Rate in Bits/Symb & Encoding Millisec & Decoding Millisec \\
        \midrule
            \texttt{alice29.txt}  &   70148 & 3.689839501870608 & 35.2187500000000 & 16.9062500000000 \\
            \texttt{asyoulik.txt} &   62748 & 4.010129494563784 & 30.5937500000000 & 14.6562500000000 \\
            \texttt{bible.txt}    & 1501134 & 2.967113637621456 & 303.250000000000 & 118.062500000000 \\
            \texttt{cp.html}      &   14948 & 4.860545461935536 & 8.12500000000000 & 7.12500000000000 \\
            \texttt{E.coli}       & 1342680 & 2.315619280443400 & 318.437500000000 & 77.7500000000000 \\
            \texttt{fields.c}     &    7084 & 5.082690582959640 & 5.21875000000000 & 4.09375000000000 \\
            \texttt{grammar.lsp}  &    2818 & 6.058586401504968 & 3.56250000000000 & 3.31250000000000 \\
            \texttt{kennedy.xls}  &  332902 & 2.586289407852824 & 163.562500000000 & 111.468750000000 \\
            \texttt{lcet10.txt}   &  185158 & 3.471002029272128 & 84.3125000000000 & 79.4687500000000 \\
            \texttt{plrabn12.txt} &  220976 & 3.668709441104384 & 102.718750000000 & 87.1875000000000 \\
            \texttt{ptt5}         &   70116 & 1.092966704077808 & 66.7812500000000 & 20.6875000000000 \\
            \texttt{sum}          &   25054 & 5.241422594142256 & 8.90625000000000 & 7.87500000000000 \\
            \texttt{world192.txt} & 1075670 & 3.479162286730808 & 206.125000000000 & 83.1562500000000 \\
            \texttt{xargs.1}      &    3584 & 6.783061272770280 & 3.81250000000000 & 5.12500000000000 \\
        \bottomrule
        \end{tabular}
        \caption{Lempel-Ziv-Welch Coder (``Markov'' Model-ish) Results in the Canterbury Corpus Test Set}
        \label{tab:lzw}
    \end{table*}

    \section{Lempel--Ziv--Welch Coding} \label{sec:lempel_ziv_welch}
        \subsection{Encoding Algorithm} \label{sec:lzw_encoding_algorithm}
        \subsection{Decoding Algorithm} \label{sec:lzw_decoding_algorithm}
        \subsection{Implementation} \label{sec:lzw_implementation}
        \subsection{Rate and Speed} \label{sec:lzw_rate_and_speed}

    \begin{table*}[t]
        \centering
        \begin{tabular}{lllll}
        \toprule
            Source's Filename & Size in Bytes & Rate in Bits/Symb & Encoding Millisec & Decoding Millisec \\
        \midrule
            \texttt{alice29.txt}  &   87653 & 4.610616152384456 & 118.375000000000 & 111.312500000000 \\
            \texttt{asyoulik.txt} &   75794 & 4.843879564463680 & 87.1562500000000 & 106.906250000000 \\
            \texttt{bible.txt}    & 2220355 & 4.388712533898368 & 867.375000000000 & 882.562500000000 \\
            \texttt{cp.html}      &   16306 & 5.302117627931552 & 42.5937500000000 & 49.0312500000000 \\
            \texttt{E.coli}       & 1173737 & 2.024255986065024 & 630.375000000000 & 633.000000000000 \\
            \texttt{fields.c}     &    7156 & 5.134349775784752 & 32.7812500000000 & 24.3437500000000 \\
            \texttt{grammar.lsp}  &    2297 & 4.938457403923672 & 12.7812500000000 & 12.7500000000000 \\
            \texttt{kennedy.xls}  &  482256 & 3.746608865892880 & 214.750000000000 & 217.937500000000 \\
            \texttt{lcet10.txt}   &  257791 & 4.832592078808864 & 185.812500000000 & 185.812500000000 \\
            \texttt{plrabn12.txt} &  274853 & 4.563191459777816 & 197.531250000000 & 187.593750000000 \\
            \texttt{ptt5}         &  117098 & 1.825321112358144 & 116.906250000000 & 149.656250000000 \\
            \texttt{sum}          &   28393 & 5.939958158995808 & 64.4687500000000 & 56.3437500000000 \\
            \texttt{world192.txt} & 1573387 & 5.088985202555184 & 588.937500000000 & 594.062500000000 \\
            \texttt{xargs.1}      &    2735 & 5.176247929973976 & 13.2812500000000 & 13.4687500000000 \\
        \bottomrule
        \end{tabular}
        \caption{Adaptive Arithmetic Coder (Stationary Model) Results in the Canterbury Corpus Test Set}
        \label{tab:aac}
    \end{table*}

    \clearpage
    \section{Adaptive Arithmetic Coding} \label{sec:adaptive_arithmetic_coding}
        \subsection{Encoding Algorithm} \label{sec:aac_encoding_algorithm}
        \subsection{Decoding Algorithm} \label{sec:aac_decoding_algorithm}
        \subsection{Implementation} \label{sec:aac_implementation}
        \subsection{Rate and Speed} \label{sec:aac_rate_and_speed}

    \appendix \onecolumn

    \clearpage
    \section{Source Code for the \texttt{entropy} Script} \label{sec:script}

    \lstinputlisting[language=Python, caption={{entropy.py}}]{share/entropy.py}

    \section{Source Code for the \texttt{liblzw} Library} \label{sec:liblzw}

    \lstinputlisting[caption={{lzwz.cc}}]{share/liblzw/lzwz.cc}
    \lstinputlisting[caption={{lzwx.cc}}]{share/liblzw/lzwx.cc}

    \lstinputlisting[caption={{lzw.hh}}]{share/liblzw/lzw.hh}
    \lstinputlisting[caption={{definitions.hh}}]{share/liblzw/definitions.hh}

    \lstinputlisting[caption={{buffer.hh}}]{share/liblzw/buffer.hh}
    \lstinputlisting[caption={{dictionary.hh}}]{share/liblzw/dictionary.hh}
    \lstinputlisting[caption={{dictionary.cc}}]{share/liblzw/dictionary.cc}
    \lstinputlisting[caption={{encoder.hh}}]{share/liblzw/encoder.hh}
    \lstinputlisting[caption={{encoder.cc}}]{share/liblzw/encoder.cc}
    \lstinputlisting[caption={{decoder.hh}}]{share/liblzw/decoder.hh}
    \lstinputlisting[caption={{decoder.cc}}]{share/liblzw/decoder.cc}

    \section{Source Code for the \texttt{libaac} Library} \label{sec:libaac}

    \lstinputlisting[caption={{aacz.cc}}]{share/libaac/aacz.cc}
    \lstinputlisting[caption={{aacx.cc}}]{share/libaac/aacx.cc}

    \lstinputlisting[caption={{aac.hh}}]{share/libaac/aac.hh}
    \lstinputlisting[caption={{configs.hh}}]{share/libaac/configs.hh}

    \lstinputlisting[caption={{statistics.hh}}]{share/libaac/statistics.hh}
    \lstinputlisting[caption={{statistics.cc}}]{share/libaac/statistics.cc}
    \lstinputlisting[caption={{encoder.hh}}]{share/libaac/encoder.hh}
    \lstinputlisting[caption={{encoder.cc}}]{share/libaac/encoder.cc}
    \lstinputlisting[caption={{decoder.hh}}]{share/libaac/decoder.hh}
    \lstinputlisting[caption={{decoder.cc}}]{share/libaac/decoder.cc}

\end{document}
